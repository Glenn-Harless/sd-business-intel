# Phase 2.5: Momentum Score, Deep Civic Context, Energy & Business Age — Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add momentum scoring (0-100 composite), granular civic context (crime breakdown, permit timelines, 311 services, energy benchmarks), and business age analysis to neighborhood/area profiles.

**Architecture:** Layered build — ingest 3 new sibling parquets, compute 4 new aggregated parquets (business age + momentum), extend query layer with 5 new + 4 modified functions, add 5 API endpoints + 5 MCP tools, integrate everything into existing dashboard views (no new tabs).

**Tech Stack:** DuckDB, Python, FastAPI, Streamlit, Pydantic, FastMCP

---

### Task 1: Ingest New Civic Data Sources

**Files:**
- Modify: `pipeline/ingest_civic.py:14-21` (CIVIC_SOURCES dict)

**Context:** CIVIC_SOURCES is a dict mapping sibling project relative paths to local filenames. The `ingest()` function copies files from sibling `../sd-*` directories into `data/aggregated/`.

**What to do:** Add 3 new entries to the CIVIC_SOURCES dict at line 14:

```python
CIVIC_SOURCES: dict[str, str] = {
    # existing 6 entries...
    "sd-housing-permits/data/aggregated/construction_by_zip.parquet": "civic_permits.parquet",
    "sd-climate-action/data/aggregated/solar_by_zip.parquet": "civic_solar.parquet",
    "sd-public-safety/data/aggregated/crime_by_zip.parquet": "civic_crime.parquet",
    "sd-get-it-done/data/aggregated/response_by_neighborhood.parquet": "civic_311.parquet",
    "sd-get-it-done/data/aggregated/monthly_trends.parquet": "civic_311_monthly.parquet",
    "sd-homelessness/data/aggregated/pit_geography.parquet": "civic_homelessness.parquet",
    # NEW for Phase 2.5:
    "sd-get-it-done/data/aggregated/top_problem_types.parquet": "civic_311_services.parquet",
    "sd-housing-permits/data/aggregated/approval_timelines.parquet": "civic_permit_timelines.parquet",
    "sd-climate-action/data/aggregated/energy_by_zip_annual.parquet": "civic_energy.parquet",
}
```

**Verify:** Run `uv run python -m pipeline.build --force` and confirm 3 new files appear in `data/aggregated/`:
- `civic_311_services.parquet` (~47 rows: service_name, total_requests, closed_requests, avg_resolution_days, median_resolution_days, close_rate_pct)
- `civic_permit_timelines.parquet` (~2,483 rows: year, approval_type_clean, zip_code, permit_count, median_days, avg_days, p90_days)
- `civic_energy.parquet` (~1,638 rows: zip_code, year, total_kwh, elec_customers, avg_kwh_per_customer, total_thm, gas_customers)

**Commit:** `git add pipeline/ingest_civic.py && git commit -m "feat: ingest 311 services, permit timelines, energy data"`

---

### Task 2: Build Business Age Aggregation

**Files:**
- Modify: `pipeline/transform.py:388-473` (_build_aggregates), and add new `_build_business_age()` function

**Context:** `_build_aggregates()` at line 388 orchestrates all aggregation. New functions should be added after `_build_trends()` (line 473). The `ZIP_TO_AREA` dict is at lines 137-244. Business dates are in `data/processed/businesses.parquet` with columns: `start_date` (DATE), `category` (VARCHAR), `zip_code` (VARCHAR), `status` (VARCHAR).

**What to do:**

1. Add a `_build_business_age(con)` function after `_build_trends()` (after line 739). This function:

```python
def _build_business_age(con) -> None:
    """Build business age statistics by zip+category and area+category."""
    biz_path = str(_PROCESSED / "businesses.parquet")
    out_dir = _AGGREGATED

    # Per zip + category
    con.execute(f"""
        COPY (
            SELECT
                zip_code,
                category,
                COUNT(*) AS business_count,
                ROUND(MEDIAN(DATEDIFF('day', start_date, CURRENT_DATE) / 365.25), 1) AS median_age_years,
                ROUND(AVG(DATEDIFF('day', start_date, CURRENT_DATE) / 365.25), 1) AS avg_age_years,
                ROUND(100.0 * SUM(CASE WHEN DATEDIFF('day', start_date, CURRENT_DATE) < 730 THEN 1 ELSE 0 END) / COUNT(*), 1) AS pct_under_2yr,
                ROUND(100.0 * SUM(CASE WHEN DATEDIFF('day', start_date, CURRENT_DATE) > 3652 THEN 1 ELSE 0 END) / COUNT(*), 1) AS pct_over_10yr
            FROM '{biz_path}'
            WHERE start_date IS NOT NULL AND status = 'active'
            GROUP BY zip_code, category
            HAVING COUNT(*) >= 3
        ) TO '{out_dir / "business_age_stats.parquet"}' (FORMAT PARQUET)
    """)

    # Per area + category (using ZIP_TO_AREA)
    za_values = ", ".join(
        f"('{z}', '{a}')" for z, a in ZIP_TO_AREA.items()
    )
    con.execute(f"""
        COPY (
            WITH za AS (SELECT * FROM (VALUES {za_values}) AS t(zip_code, area)),
                 ages AS (
                     SELECT
                         b.zip_code,
                         b.category,
                         DATEDIFF('day', b.start_date, CURRENT_DATE) / 365.25 AS age_years
                     FROM '{biz_path}' b
                     WHERE b.start_date IS NOT NULL AND b.status = 'active'
                 )
            SELECT
                COALESCE(za.area, 'Other') AS area,
                a.category,
                COUNT(*) AS business_count,
                ROUND(MEDIAN(a.age_years), 1) AS median_age_years,
                ROUND(AVG(a.age_years), 1) AS avg_age_years,
                ROUND(100.0 * SUM(CASE WHEN a.age_years < 2 THEN 1 ELSE 0 END) / COUNT(*), 1) AS pct_under_2yr,
                ROUND(100.0 * SUM(CASE WHEN a.age_years > 10 THEN 1 ELSE 0 END) / COUNT(*), 1) AS pct_over_10yr
            FROM ages a
            LEFT JOIN za ON a.zip_code = za.zip_code
            GROUP BY COALESCE(za.area, 'Other'), a.category
            HAVING COUNT(*) >= 3
        ) TO '{out_dir / "business_age_by_area.parquet"}' (FORMAT PARQUET)
    """)
```

2. Call `_build_business_age(con)` in `_build_aggregates()` after the `_build_trends(con)` call (after line 473).

**Verify:** Run `uv run python -m pipeline.build --force` and check:
```python
uv run python3 -c "
import duckdb; con = duckdb.connect()
df = con.execute(\"SELECT * FROM 'data/aggregated/business_age_stats.parquet' WHERE zip_code='92101' ORDER BY business_count DESC LIMIT 5\").fetchdf()
print(df)
df2 = con.execute(\"SELECT * FROM 'data/aggregated/business_age_by_area.parquet' WHERE area='Downtown' ORDER BY business_count DESC LIMIT 5\").fetchdf()
print(df2)
"
```

**Commit:** `git add pipeline/transform.py && git commit -m "feat: add business age aggregation by zip and area"`

---

### Task 3: Build Momentum Score Aggregation

**Files:**
- Modify: `pipeline/transform.py` — add `_build_momentum(con)` function and call it from `_build_aggregates()`

**Context:** Momentum uses YoY changes from 4 existing datasets. All are in `data/aggregated/`:
- `trend_business_formation.parquet`: zip_code, year, new_businesses
- `civic_permits.parquet`: zip_code, year, permit_count, total_valuation, total_du
- `civic_crime.parquet`: zip_code, city, year, crime_against, count
- `civic_solar.parquet`: zip_code, year, solar_count, total_valuation, median_approval_days

**Algorithm:**
1. For each zip, compute YoY % change (2025 vs 2024) for each metric
2. Compute percentile rank (0-1) of each zip's YoY across all zips
3. Multiply each percentile by 25 → 0-25 per component
4. Crime is inverted (decrease = higher score)
5. Missing metrics get neutral score (12.5)
6. Sum 4 components → 0-100 composite
7. Area-level: population-weighted average of zip scores

**What to do:** Add `_build_momentum(con)` function:

```python
def _build_momentum(con) -> None:
    """Build momentum scores (0-100) per zip and area."""
    out_dir = _AGGREGATED
    biz_path = str(out_dir / "trend_business_formation.parquet")
    permits_path = str(out_dir / "civic_permits.parquet")
    crime_path = str(out_dir / "civic_crime.parquet")
    solar_path = str(out_dir / "civic_solar.parquet")
    demo_path = str(out_dir / "demographics_by_zip.parquet")

    con.execute(f"""
        COPY (
            WITH
            biz_yoy AS (
                SELECT zip_code,
                    MAX(CASE WHEN year = 2025 THEN new_businesses END) AS cur,
                    MAX(CASE WHEN year = 2024 THEN new_businesses END) AS prev
                FROM '{biz_path}'
                WHERE year IN (2024, 2025)
                GROUP BY zip_code
            ),
            permit_yoy AS (
                SELECT zip_code,
                    SUM(CASE WHEN year = 2025 THEN permit_count ELSE 0 END) AS cur,
                    SUM(CASE WHEN year = 2024 THEN permit_count ELSE 0 END) AS prev
                FROM '{permits_path}'
                WHERE year IN (2024, 2025)
                GROUP BY zip_code
            ),
            crime_yoy AS (
                SELECT zip_code,
                    SUM(CASE WHEN year = 2025 THEN count ELSE 0 END) AS cur,
                    SUM(CASE WHEN year = 2024 THEN count ELSE 0 END) AS prev
                FROM '{crime_path}'
                WHERE year IN (2024, 2025)
                GROUP BY zip_code
            ),
            solar_yoy AS (
                SELECT zip_code,
                    SUM(CASE WHEN year = 2025 THEN solar_count ELSE 0 END) AS cur,
                    SUM(CASE WHEN year = 2024 THEN solar_count ELSE 0 END) AS prev
                FROM '{solar_path}'
                WHERE year IN (2024, 2025)
                GROUP BY zip_code
            ),
            all_zips AS (
                SELECT DISTINCT zip_code FROM '{demo_path}'
            ),
            raw AS (
                SELECT
                    z.zip_code,
                    CASE WHEN b.prev > 0 THEN 100.0 * (b.cur - b.prev) / b.prev ELSE NULL END AS biz_yoy,
                    CASE WHEN p.prev > 0 THEN 100.0 * (p.cur - p.prev) / p.prev ELSE NULL END AS permit_yoy,
                    CASE WHEN c.prev > 0 THEN -100.0 * (c.cur - c.prev) / c.prev ELSE NULL END AS crime_yoy,
                    CASE WHEN s.prev > 0 THEN 100.0 * (s.cur - s.prev) / s.prev ELSE NULL END AS solar_yoy
                FROM all_zips z
                LEFT JOIN biz_yoy b ON z.zip_code = b.zip_code
                LEFT JOIN permit_yoy p ON z.zip_code = p.zip_code
                LEFT JOIN crime_yoy c ON z.zip_code = c.zip_code
                LEFT JOIN solar_yoy s ON z.zip_code = s.zip_code
            ),
            ranked AS (
                SELECT
                    zip_code,
                    biz_yoy,
                    permit_yoy,
                    crime_yoy,
                    solar_yoy,
                    COALESCE(25.0 * PERCENT_RANK() OVER (ORDER BY biz_yoy ASC NULLS FIRST), 12.5) AS biz_score,
                    COALESCE(25.0 * PERCENT_RANK() OVER (ORDER BY permit_yoy ASC NULLS FIRST), 12.5) AS permit_score,
                    COALESCE(25.0 * PERCENT_RANK() OVER (ORDER BY crime_yoy ASC NULLS FIRST), 12.5) AS crime_score,
                    COALESCE(25.0 * PERCENT_RANK() OVER (ORDER BY solar_yoy ASC NULLS FIRST), 12.5) AS solar_score
                FROM raw
            )
            SELECT
                zip_code,
                ROUND(biz_score + permit_score + crime_score + solar_score, 1) AS momentum_score,
                ROUND(biz_yoy, 1) AS biz_formation_yoy,
                ROUND(permit_yoy, 1) AS permit_yoy,
                ROUND(crime_yoy, 1) AS crime_yoy,
                ROUND(solar_yoy, 1) AS solar_yoy
            FROM ranked
            ORDER BY momentum_score DESC
        ) TO '{out_dir / "momentum_scores.parquet"}' (FORMAT PARQUET)
    """)

    # Area-level: population-weighted average
    za_values = ", ".join(
        f"('{z}', '{a}')" for z, a in ZIP_TO_AREA.items()
    )
    con.execute(f"""
        COPY (
            WITH za AS (SELECT * FROM (VALUES {za_values}) AS t(zip_code, area))
            SELECT
                COALESCE(za.area, 'Other') AS area,
                ROUND(SUM(m.momentum_score * CAST(d.population AS DOUBLE)) / NULLIF(SUM(CAST(d.population AS DOUBLE)), 0), 1) AS momentum_score,
                ROUND(SUM(COALESCE(m.biz_formation_yoy, 0) * CAST(d.population AS DOUBLE)) / NULLIF(SUM(CAST(d.population AS DOUBLE)), 0), 1) AS biz_formation_yoy,
                ROUND(SUM(COALESCE(m.permit_yoy, 0) * CAST(d.population AS DOUBLE)) / NULLIF(SUM(CAST(d.population AS DOUBLE)), 0), 1) AS permit_yoy,
                ROUND(SUM(COALESCE(m.crime_yoy, 0) * CAST(d.population AS DOUBLE)) / NULLIF(SUM(CAST(d.population AS DOUBLE)), 0), 1) AS crime_yoy,
                ROUND(SUM(COALESCE(m.solar_yoy, 0) * CAST(d.population AS DOUBLE)) / NULLIF(SUM(CAST(d.population AS DOUBLE)), 0), 1) AS solar_yoy
            FROM '{out_dir / "momentum_scores.parquet"}' m
            JOIN '{demo_path}' d ON m.zip_code = d.zip_code
            LEFT JOIN za ON m.zip_code = za.zip_code
            GROUP BY COALESCE(za.area, 'Other')
            ORDER BY momentum_score DESC
        ) TO '{out_dir / "momentum_by_area.parquet"}' (FORMAT PARQUET)
    """)
```

Call `_build_momentum(con)` in `_build_aggregates()` after `_build_business_age(con)`.

**Verify:** Run `uv run python -m pipeline.build --force` and check:
```python
uv run python3 -c "
import duckdb; con = duckdb.connect()
df = con.execute(\"SELECT * FROM 'data/aggregated/momentum_scores.parquet' ORDER BY momentum_score DESC LIMIT 10\").fetchdf()
print(df)
df2 = con.execute(\"SELECT * FROM 'data/aggregated/momentum_by_area.parquet' ORDER BY momentum_score DESC LIMIT 10\").fetchdf()
print(df2)
"
```

Expected: ~82 zip rows with scores 0-100, ~24 area rows. Verify scores are distributed (not all the same).

**Commit:** `git add pipeline/transform.py && git commit -m "feat: add momentum score aggregation by zip and area"`

---

### Task 4: New Query Functions (5 functions)

**Files:**
- Modify: `api/queries.py` — add 5 new functions after `_add_yoy()` (after line 1128)

**What to do:** Add these 5 query functions:

```python
def get_momentum_scores(limit: int = 20) -> list[dict]:
    """Get zip codes ranked by momentum score."""
    path = _q("data/aggregated/momentum_scores.parquet")
    return _run(f"""
        SELECT * FROM '{path}'
        ORDER BY momentum_score DESC
        LIMIT $1
    """, [min(limit, 100)])


def get_area_momentum(limit: int = 20) -> list[dict]:
    """Get areas ranked by momentum score."""
    path = _q("data/aggregated/momentum_by_area.parquet")
    return _run(f"""
        SELECT * FROM '{path}'
        ORDER BY momentum_score DESC
        LIMIT $1
    """, [min(limit, 100)])


def get_business_age(zip_code: str) -> list[dict]:
    """Get business age stats by category for a zip code."""
    path = _q("data/aggregated/business_age_stats.parquet")
    rows = _run(f"""
        SELECT category, business_count, median_age_years, avg_age_years,
               pct_under_2yr, pct_over_10yr
        FROM '{path}'
        WHERE zip_code = $1
        ORDER BY business_count DESC
        LIMIT 15
    """, [zip_code])
    return [{k: _clean(v) for k, v in r.items()} for r in rows]


def get_area_business_age(area: str) -> list[dict]:
    """Get business age stats by category for an area."""
    path = _q("data/aggregated/business_age_by_area.parquet")
    rows = _run(f"""
        SELECT category, business_count, median_age_years, avg_age_years,
               pct_under_2yr, pct_over_10yr
        FROM '{path}'
        WHERE area = $1
        ORDER BY business_count DESC
        LIMIT 15
    """, [area])
    return [{k: _clean(v) for k, v in r.items()} for r in rows]


def get_311_services() -> list[dict]:
    """Get city-wide 311 service type breakdown."""
    path = _q("data/aggregated/civic_311_services.parquet")
    rows = _run(f"""
        SELECT service_name, total_requests, closed_requests,
               ROUND(avg_resolution_days, 1) AS avg_resolution_days,
               ROUND(median_resolution_days, 1) AS median_resolution_days,
               ROUND(close_rate_pct, 1) AS close_rate_pct
        FROM '{path}'
        ORDER BY total_requests DESC
    """)
    return [{k: _clean(v) for k, v in r.items()} for r in rows]
```

**Verify:** Run each function:
```python
uv run python3 -c "
from api.queries import get_momentum_scores, get_area_momentum, get_business_age, get_area_business_age, get_311_services
print('momentum:', len(get_momentum_scores(5)))
print('area momentum:', len(get_area_momentum(5)))
print('biz age 92101:', len(get_business_age('92101')))
print('area biz age:', len(get_area_business_age('Downtown')))
print('311 services:', len(get_311_services()))
"
```

**Commit:** `git add api/queries.py && git commit -m "feat: add momentum, business age, 311 services query functions"`

---

### Task 5: Modify Profile Queries — Crime Breakdown, Energy, Permit Timelines, Momentum

**Files:**
- Modify: `api/queries.py` — `get_neighborhood_profile()` (lines 140-299) and `get_area_profile()` (lines 781-867)

**Context:** Both profile functions return a dict with sections: demographics, business_landscape, civic_signals, comparison_to_avg, narrative. We need to add new fields to civic_signals and add new top-level sections.

**What to do for `get_neighborhood_profile()`:**

Add after the existing civic signals queries (around line 230), before the return statement:

1. **Crime breakdown** — query civic_crime.parquet for latest year's Person/Property/Society split:
```python
    # Crime breakdown by type for this zip
    crime_path = _q("data/aggregated/civic_crime.parquet")
    crime_breakdown = _run(f"""
        SELECT crime_against, SUM(count) AS count
        FROM '{crime_path}'
        WHERE zip_code = $1 AND year = (SELECT MAX(year) FROM '{crime_path}' WHERE zip_code = $1)
        GROUP BY crime_against
        ORDER BY count DESC
    """, [zip_code])
```

2. **Energy benchmark** — query civic_energy.parquet for latest year:
```python
    # Energy benchmark
    energy_path = _q("data/aggregated/civic_energy.parquet")
    energy = _run_one(f"""
        SELECT avg_kwh_per_customer, total_kwh, elec_customers
        FROM '{energy_path}'
        WHERE zip_code = $1
        ORDER BY year DESC LIMIT 1
    """, [zip_code])
```

3. **Permit timelines** — query civic_permit_timelines.parquet for this zip, latest year, top 3 types:
```python
    # Permit approval timelines
    pt_path = _q("data/aggregated/civic_permit_timelines.parquet")
    permit_timelines = _run(f"""
        SELECT approval_type_clean AS permit_type, permit_count,
               ROUND(median_days, 0) AS median_days
        FROM '{pt_path}'
        WHERE zip_code = $1
          AND year = (SELECT MAX(year) FROM '{pt_path}' WHERE zip_code = $1 AND permit_count > 0)
          AND permit_count > 0
        ORDER BY permit_count DESC
        LIMIT 3
    """, [zip_code])
```

4. **Momentum score** — query momentum_scores.parquet:
```python
    # Momentum score
    mom_path = _q("data/aggregated/momentum_scores.parquet")
    momentum = _run_one(f"SELECT * FROM '{mom_path}' WHERE zip_code = $1", [zip_code])
```

5. **Business age** — query business_age_stats.parquet:
```python
    # Business age stats
    age_path = _q("data/aggregated/business_age_stats.parquet")
    age_stats = _run(f"""
        SELECT category, business_count, median_age_years, avg_age_years,
               pct_under_2yr, pct_over_10yr
        FROM '{age_path}'
        WHERE zip_code = $1
        ORDER BY business_count DESC LIMIT 5
    """, [zip_code])
```

Then add to the return dict:
```python
    # Add to civic_signals section:
    "crime_breakdown": [{k: _clean(v) for k, v in c.items()} for c in crime_breakdown],
    "energy": {k: _clean(v) for k, v in energy.items()} if energy else None,
    "permit_timelines": [{k: _clean(v) for k, v in p.items()} for p in permit_timelines],

    # Add as top-level sections:
    "momentum": {k: _clean(v) for k, v in momentum.items()} if momentum else None,
    "business_age": [{k: _clean(v) for k, v in a.items()} for a in age_stats],
```

**Do the same for `get_area_profile()`** — same pattern but:
- Crime: filter by zips in the area (join with ZIP_TO_AREA or use area_profile.zip_codes)
- Energy: average across area zips for latest year
- Permit timelines: aggregate across area zips
- Momentum: query momentum_by_area.parquet instead
- Business age: query business_age_by_area.parquet instead

**Verify:** Test both:
```python
uv run python3 -c "
from api.queries import get_neighborhood_profile, get_area_profile
p = get_neighborhood_profile('92101')
print('crime_breakdown:', p.get('civic_signals', {}).get('crime_breakdown'))
print('energy:', p.get('civic_signals', {}).get('energy'))
print('permit_timelines:', p.get('civic_signals', {}).get('permit_timelines'))
print('momentum:', p.get('momentum'))
print('business_age:', len(p.get('business_age', [])))

a = get_area_profile('Downtown')
print('area momentum:', a.get('momentum'))
print('area business_age:', len(a.get('business_age', [])))
"
```

**Commit:** `git add api/queries.py && git commit -m "feat: add crime breakdown, energy, permit timelines, momentum, business age to profiles"`

---

### Task 6: Update Rankings to Support Momentum Score

**Files:**
- Modify: `api/queries.py` — `_PERCENTILE_METRICS` (line 20), `get_rankings()` (line 628), `get_area_rankings()` (line 913)

**What to do:**

1. Add `"momentum_score": "DESC"` to `_PERCENTILE_METRICS` dict (line 20).

2. In `get_rankings()` (line 628): The function reads from `neighborhood_profile.parquet` and sorts by a metric. Momentum score is in a separate file (`momentum_scores.parquet`). Add a JOIN when `sort_by == "momentum_score"`:
   - If sort_by is `momentum_score`, JOIN momentum_scores on zip_code and use that as sort_value
   - Otherwise, existing logic unchanged

3. In `get_area_rankings()` (line 913): Same pattern but use `momentum_by_area.parquet` and JOIN on area.

**Verify:**
```python
uv run python3 -c "
from api.queries import get_rankings, get_area_rankings
r = get_rankings('momentum_score', 5)
print('zip rankings by momentum:', [(x.get('zip_code'), x.get('sort_value')) for x in r])
r2 = get_area_rankings('momentum_score', 5)
print('area rankings by momentum:', [(x.get('area'), x.get('sort_value')) for x in r2])
"
```

**Commit:** `git add api/queries.py && git commit -m "feat: add momentum_score to rankings"`

---

### Task 7: Update Narrative Functions

**Files:**
- Modify: `api/queries.py` — `_build_narrative()` (line 347) and `_build_area_narrative()` (line 500)

**Context:** Narratives are pure f-string logic. They describe the zip/area compared to city averages. Add momentum and business age sentences.

**What to do:**

At the end of `_build_narrative()`, before the return, add:
```python
    # Momentum
    momentum_score = extra.get("momentum_score")
    if momentum_score is not None:
        if momentum_score >= 60:
            parts.append(f"momentum score: {momentum_score}/100 (strong growth trajectory)")
        elif momentum_score >= 40:
            parts.append(f"momentum score: {momentum_score}/100 (moderate growth)")
        else:
            parts.append(f"momentum score: {momentum_score}/100 (slower growth period)")

    # Business age
    top_age = extra.get("top_business_age")
    if top_age:
        parts.append(f"most common business type ({top_age['category']}) has median age of {top_age['median_age_years']} years")
```

Note: The narrative functions need to receive the momentum and business age data. Pass them via an `extra` dict parameter, or look them up inline. Check how the functions are called and choose the simplest approach — likely adding the data to the `row` dict before calling `_build_narrative()`.

Do the same for `_build_area_narrative()`.

**Verify:** Check narrative output includes momentum and age:
```python
uv run python3 -c "
from api.queries import get_neighborhood_profile
p = get_neighborhood_profile('92101')
print(p.get('narrative', ''))
"
```

**Commit:** `git add api/queries.py && git commit -m "feat: add momentum and business age to profile narratives"`

---

### Task 8: Add Pydantic Models

**Files:**
- Modify: `api/models.py` — add new models and update existing ones

**What to do:**

1. Add `areas: list[str] = []` to `FilterOptions` (line 8).

2. Add new models:

```python
class CrimeBreakdown(BaseModel):
    crime_against: str = ""
    count: int = 0

class EnergyBenchmark(BaseModel):
    avg_kwh_per_customer: float | None = None
    total_kwh: float | None = None
    elec_customers: int | None = None

class PermitTimeline(BaseModel):
    permit_type: str = ""
    permit_count: int = 0
    median_days: float | None = None

class MomentumScore(BaseModel):
    zip_code: str | None = None
    area: str | None = None
    momentum_score: float | None = None
    biz_formation_yoy: float | None = None
    permit_yoy: float | None = None
    crime_yoy: float | None = None
    solar_yoy: float | None = None

class BusinessAge(BaseModel):
    category: str = ""
    business_count: int = 0
    median_age_years: float | None = None
    avg_age_years: float | None = None
    pct_under_2yr: float | None = None
    pct_over_10yr: float | None = None

class ServiceType(BaseModel):
    service_name: str = ""
    total_requests: int = 0
    closed_requests: int = 0
    avg_resolution_days: float | None = None
    median_resolution_days: float | None = None
    close_rate_pct: float | None = None
```

3. Update `CivicSignals` to include new fields:
```python
class CivicSignals(BaseModel):
    # existing fields...
    crime_breakdown: list[CrimeBreakdown] = []
    energy: EnergyBenchmark | None = None
    permit_timelines: list[PermitTimeline] = []
```

4. Update `NeighborhoodProfile` and `AreaProfile` to include:
```python
    momentum: MomentumScore | None = None
    business_age: list[BusinessAge] = []
```

**Commit:** `git add api/models.py && git commit -m "feat: add Phase 2.5 Pydantic models"`

---

### Task 9: Add API Endpoints

**Files:**
- Modify: `api/main.py` — add 5 new endpoints after existing ones (after line 152)

**What to do:**

```python
@app.get("/momentum")
def momentum(limit: int = 20):
    return queries.get_momentum_scores(limit)

@app.get("/area-momentum")
def area_momentum(limit: int = 20):
    return queries.get_area_momentum(limit)

@app.get("/business-age/{zip_code}")
def business_age(zip_code: str):
    return queries.get_business_age(zip_code)

@app.get("/area-business-age/{area}")
def area_business_age(area: str):
    return queries.get_area_business_age(area)

@app.get("/311-services")
def services_311():
    return queries.get_311_services()
```

**Verify:** `uv run uvicorn api.main:app --reload` then test:
- `curl localhost:8000/momentum?limit=3`
- `curl localhost:8000/area-momentum?limit=3`
- `curl localhost:8000/business-age/92101`
- `curl "localhost:8000/area-business-age/Downtown"`
- `curl localhost:8000/311-services`

**Commit:** `git add api/main.py && git commit -m "feat: add 5 Phase 2.5 API endpoints"`

---

### Task 10: Add MCP Tools

**Files:**
- Modify: `api/mcp_server.py` — add 5 new tools after existing ones (after line 158)

**Context:** MCP tools are thin wrappers using `@mcp.tool()` decorator. Follow the existing pattern in the file.

**What to do:**

```python
@mcp.tool()
def get_momentum_scores(limit: int = 20) -> list[dict]:
    """Rank San Diego zip codes by momentum score (0-100).

    Composite score combining business formation, permit activity,
    crime trends, and solar adoption year-over-year changes."""
    return queries.get_momentum_scores(limit)

@mcp.tool()
def get_area_momentum(limit: int = 20) -> list[dict]:
    """Rank San Diego areas by momentum score (0-100).

    Population-weighted composite of zip-level momentum scores."""
    return queries.get_area_momentum(limit)

@mcp.tool()
def get_business_age(zip_code: str) -> list[dict]:
    """Get business age statistics by category for a zip code.

    Shows median age, avg age, pct under 2 years, pct over 10 years."""
    return queries.get_business_age(zip_code)

@mcp.tool()
def get_area_business_age(area: str) -> list[dict]:
    """Get business age statistics by category for an area.

    Shows median age, avg age, pct under 2 years, pct over 10 years."""
    return queries.get_area_business_age(area)

@mcp.tool()
def get_311_services() -> list[dict]:
    """Get city-wide 311 service type breakdown.

    Returns 47 service types with total requests, resolution times,
    and close rates. City-wide data (not zip-level)."""
    return queries.get_311_services()
```

**Verify:** Test MCP tools work:
```python
uv run python3 -c "
from api.mcp_server import mcp
# Tools are registered; verify they exist
print([t.name for t in mcp._tool_manager.list_tools()])
"
```

**Commit:** `git add api/mcp_server.py && git commit -m "feat: add 5 Phase 2.5 MCP tools"`

---

### Task 11: Dashboard — Profile Enhancements

**Files:**
- Modify: `dashboard/app.py`

**Context:** The dashboard uses `_render_zip_explorer()` (line 534) for zip profiles and has area profile rendering in the explorer tab (line 712). Both call `queries.get_neighborhood_profile()` or `queries.get_area_profile()` and render metric cards.

**What to do:**

1. **Add cached loaders** for new data (add near line 288 with other loaders):
```python
@st.cache_data(ttl=3600)
def _load_momentum(zip_code):
    return queries.get_momentum_scores(100)

@st.cache_data(ttl=3600)
def _load_311_services():
    return queries.get_311_services()
```

2. **Momentum score card** — In both zip and area profile views, add a prominent metric card showing the momentum score (0-100) with color coding:
   - Green (>60): strong growth
   - Yellow (30-60): moderate
   - Red (<30): slower period
   - Show expandable breakdown with the 4 YoY components
   - Place it after the main demographics metrics, before business landscape

3. **Crime breakdown** — In civic signals section, replace the single `crime_count` number with a small horizontal bar chart (plotly) showing Person / Property / Society counts. Keep crime_count as headline.

4. **Energy benchmark** — Add `avg kWh/customer` metric card to civic signals section. Compare to city average (compute from civic_energy.parquet city-wide mean).

5. **Permit approval speed** — Add "median approval days" metric to civic signals with the top 3 permit types listed below it.

6. **Business age section** — New section after business landscape:
   - Show top 5 categories with median age
   - Label categories: <2yr median = "emerging", 2-5yr = "growing", 5-8yr = "maturing", >8yr = "established"
   - Use a horizontal bar chart colored by maturity label

7. **311 services panel** — Add an expander in civic signals: "311 service breakdown (city-wide)". Inside, show top 10 service types as a table with columns: service, requests, median days, close rate.

**Important dashboard patterns to follow:**
- All text/labels lowercase
- Use `key=` params on all widgets (prefix `z_` for zip mode, `a_` for area mode)
- Use `_fmt_metric()` for metric display
- Use `_valid()` to check for None/NaN values before displaying
- Import from `api.queries` directly (no duplicated SQL)

**Verify:** Run `uv run streamlit run dashboard/app.py` and check:
- Zip profile shows momentum score, crime breakdown chart, energy metric, permit timelines, business age section
- Area profile shows the same
- 311 services expander works

**Commit:** `git add dashboard/app.py && git commit -m "feat: add momentum, civic context, energy, business age to dashboard profiles"`

---

### Task 12: Dashboard — Momentum in Rankings

**Files:**
- Modify: `dashboard/app.py` — rankings tab (around line 1013)

**Context:** The rankings tab has a metric dropdown. Rankings are loaded via `_load_rankings()` and `_load_area_rankings()` which call `queries.get_rankings()` and `queries.get_area_rankings()`.

**What to do:**

Add `"momentum_score"` to the metric options list in the rankings dropdown. The existing ranking loaders and display logic should handle it automatically since Task 6 added momentum_score support to the query functions.

Verify the display label is clean — the rankings tab formats metric names for display. Ensure "momentum_score" displays as "momentum score" (replace underscores with spaces).

**Verify:** Run dashboard, go to rankings tab, select "momentum score" from dropdown. Verify both zip and area rankings show momentum scores.

**Commit:** `git add dashboard/app.py && git commit -m "feat: add momentum score to rankings dropdown"`

---

### Task 13: Pipeline Rebuild and Full Verification

**Files:** None (verification only)

**What to do:**

1. Run full pipeline rebuild: `uv run python -m pipeline.build --force`
2. Verify new parquet files exist and have expected row counts:
   - `civic_311_services.parquet` (~47 rows)
   - `civic_permit_timelines.parquet` (~2,483 rows)
   - `civic_energy.parquet` (~1,638 rows)
   - `business_age_stats.parquet` (~1,200 rows)
   - `business_age_by_area.parquet` (~500 rows)
   - `momentum_scores.parquet` (~82 rows)
   - `momentum_by_area.parquet` (~24 rows)
3. Verify all query functions return data
4. Verify API endpoints respond
5. Run dashboard and test all views
6. Check for Python 3.11 f-string issues (no backslashes in f-string expressions)
7. Check for INT32 overflow in any population-weighted calculations (use BIGINT/DOUBLE casts)

**Syntax check:**
```bash
uv run python3 -c "
import py_compile
for f in ['pipeline/transform.py', 'pipeline/ingest_civic.py', 'api/queries.py', 'api/models.py', 'api/main.py', 'api/mcp_server.py', 'dashboard/app.py']:
    py_compile.compile(f, doraise=True)
    print(f'{f}: OK')
"
```

**Commit new parquets:**
```bash
git add data/aggregated/*.parquet
git commit -m "data: add Phase 2.5 aggregated parquets"
```

**Final commit if any fixes needed.**
